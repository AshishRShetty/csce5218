{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMJJXB5XOBy9b9R1SBrOMmD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AshishRShetty/csce5218/blob/main/Baseline_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GROUP 22 BASELINE MODEL\n",
        " ASHISH RATHNAKAR SHETTY (11808466)\n",
        " SIDDHIVINAYAK RAGHAVRAJU ()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "This baseline model generates captions for Flickr30k images using a CNN encoder (based on ResNet18) and a decoder built with LSTM or LSTM with attention.\n",
        "Images are preprocessed and captions are tokenized with BERT tokenizer.\n",
        "The model is trained with scheduled sampling and mixed precision for efficiency, and uses beam search during inference to generate more accurate captions.\n",
        "Performance is evaluated using BLEU-1 to BLEU-4 scores on a validation set."
      ],
      "metadata": {
        "id": "hIzoiVa2vrgF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " This cell sets up everything you need to get started. It will:\n",
        "- Install required packages (Kaggle, NLTK, Transformers, and Timm)\n",
        "- Prompt you to upload your Kaggle API credentials file (kaggle.json)\n",
        "- Download the Flickr30k dataset\n",
        "\n",
        "When prompted to upload your kaggle.json file, click the \"Choose Files\" button and select your credentials file. If you don't have one, you'll need to create a Kaggle account and generate API credentials from your account settings.\n"
      ],
      "metadata": {
        "id": "GtejDCTVsuED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Kaggle and Libraries\n",
        "!pip install kaggle nltk transformers timm\n",
        "\n",
        "# Upload kaggle.json manually\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "# Set Kaggle credentials\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download Flickr30k Dataset\n",
        "!kaggle datasets download -d hsankesara/flickr-image-dataset\n",
        "\n",
        "# Unzip dataset\n",
        "!unzip -q flickr-image-dataset.zip -d /content/flickr30k\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2263
        },
        "id": "UD5EhV2cq9dn",
        "outputId": "a9b92025-3602-466f-e38e-588c05db1891"
      },
      "execution_count": 19,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.1.31)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.4)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.3.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->timm)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->timm)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->timm)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->timm)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->timm)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->timm)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-22e20f4e-56e4-4fb0-8f5a-4317f82c484c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-22e20f4e-56e4-4fb0-8f5a-4317f82c484c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Dataset URL: https://www.kaggle.com/datasets/hsankesara/flickr-image-dataset\n",
            "License(s): CC0-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're importing all the libraries needed for data handling, model building, and evaluation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "39OkE34ptOe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from transformers import BertTokenizer\n"
      ],
      "metadata": {
        "id": "3mChkQiBsSIg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're creating a custom dataset class that loads images and their captions from Flickr30k and prepares them for model training."
      ],
      "metadata": {
        "id": "RX_XIQFTtP72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Flickr30kDataset(Dataset):\n",
        "    def __init__(self, root_dir, captions_file, tokenizer, transform=None, max_length=50):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Load captions with correct column name\n",
        "        self.captions_df = pd.read_csv(captions_file, delimiter='|')\n",
        "        self.image_to_captions = {}\n",
        "\n",
        "        for idx, row in self.captions_df.iterrows():\n",
        "            image_name = row['image_name'].strip()\n",
        "            caption = str(row[' comment']).strip()   # ✅ (space before comment)\n",
        "            if image_name not in self.image_to_captions:\n",
        "                self.image_to_captions[image_name] = []\n",
        "            self.image_to_captions[image_name].append(caption)\n",
        "\n",
        "        self.image_names = list(self.image_to_captions.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_names[idx]\n",
        "        img_path = os.path.join(self.root_dir, img_name)\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        captions = self.image_to_captions[img_name]\n",
        "        caption = random.choice(captions)\n",
        "\n",
        "        tokens = self.tokenizer.encode(\n",
        "            caption,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        ).squeeze(0)\n",
        "\n",
        "        return image, tokens\n"
      ],
      "metadata": {
        "id": "4SKccUWAsdt1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're defining how each image should be resized, converted to a tensor, and normalized before feeding it into the model."
      ],
      "metadata": {
        "id": "C6KVjbeatXSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data transformations for images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "metadata": {
        "id": "-inqpE43sfjf"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're loading the tokenizer, setting up the dataset and dataloaders, splitting into train and validation sets, and preparing your device (CPU or GPU) for training."
      ],
      "metadata": {
        "id": "L3wy2QyZtgL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Dataset paths\n",
        "root_dir = '/content/flickr30k/flickr30k_images/flickr30k_images'  # careful double folder\n",
        "captions_file = '/content/flickr30k/flickr30k_images/results.csv'\n",
        "\n",
        "# Create dataset\n",
        "dataset = Flickr30kDataset(\n",
        "    root_dir=root_dir,\n",
        "    captions_file=captions_file,\n",
        "    tokenizer=tokenizer,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# Train/Validation split\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "# Device setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f\"✅ Data Loaded Successfully on {device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "DN0wo_MHsgX1",
        "outputId": "2f30e6ca-f39b-4504-aba7-a6e8206acd27"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Data Loaded Successfully on cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're building an encoder using ResNet18 and a decoder using an LSTM, then initializing them and moving them to your device (CPU or GPU)."
      ],
      "metadata": {
        "id": "ILidghvEtm0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder CNN\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = models.resnet18(pretrained=True)\n",
        "        modules = list(resnet.children())[:-1]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
        "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
        "\n",
        "    def forward(self, images):\n",
        "        with torch.no_grad():\n",
        "            features = self.resnet(images)\n",
        "        features = features.view(features.size(0), -1)\n",
        "        features = self.bn(self.linear(features))\n",
        "        return features\n",
        "\n",
        "# Decoder RNN\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        embeddings = self.embed(captions)\n",
        "        inputs = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
        "        hiddens, _ = self.lstm(inputs)\n",
        "        outputs = self.linear(hiddens)\n",
        "        return outputs\n",
        "\n",
        "# Initialize Encoder and Decoder\n",
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "vocab_size = tokenizer.vocab_size\n",
        "\n",
        "encoder = EncoderCNN(embed_size).to(device)\n",
        "decoder = DecoderRNN(embed_size, hidden_size, vocab_size).to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "uCq_duwGteTj",
        "outputId": "2d75f367-1e50-4ffc-fbc8-65fdfeed3f13"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 174MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're setting up the loss function and optimizer to train both the encoder and decoder together.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8D_1HxmUtvCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n",
        "optimizer = optim.Adam(params, lr=1e-3)\n"
      ],
      "metadata": {
        "id": "HLcTkzittoH6"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're creating a folder called checkpoints to save your model files during training."
      ],
      "metadata": {
        "id": "JbrBgkUat2Pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the checkpoints folder if not exists\n",
        "!mkdir -p /content/checkpoints\n"
      ],
      "metadata": {
        "id": "yEwjoS-6uVsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're defining a function to generate a caption for a given image using the trained encoder and decoder models."
      ],
      "metadata": {
        "id": "Kg7c0JlguBvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_caption(encoder, decoder, image, tokenizer, max_length=50):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        feature = encoder(image.unsqueeze(0).to(device))\n",
        "\n",
        "        output_caption = [tokenizer.cls_token_id]  # Start token\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            current_input = torch.tensor(output_caption).unsqueeze(0).to(device)  # [1, seq_len]\n",
        "            outputs = decoder(feature, current_input)\n",
        "\n",
        "            outputs = outputs.squeeze(0)\n",
        "            predicted_id = outputs.argmax(1)[-1].item()\n",
        "\n",
        "            output_caption.append(predicted_id)\n",
        "\n",
        "            if predicted_id == tokenizer.sep_token_id:  # End token\n",
        "                break\n",
        "\n",
        "    caption = tokenizer.decode(output_caption, skip_special_tokens=True)\n",
        "    return caption\n"
      ],
      "metadata": {
        "id": "TllYfVvr0uXf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're creating an attention mechanism that helps the model focus on important parts of the image while generating each word.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3tDfxLzfut0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, feature_dim, decoder_dim, attention_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.feature_attn = nn.Linear(feature_dim, attention_dim)\n",
        "        self.hidden_attn = nn.Linear(decoder_dim, attention_dim)\n",
        "        self.full_attn = nn.Linear(attention_dim, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, features, hidden_state):\n",
        "        att1 = self.feature_attn(features)          # (batch_size, num_pixels, attention_dim)\n",
        "        att2 = self.hidden_attn(hidden_state)        # (batch_size, attention_dim)\n",
        "        att2 = att2.unsqueeze(1)                     # (batch_size, 1, attention_dim)\n",
        "        att = self.relu(att1 + att2)\n",
        "        att = self.full_attn(att).squeeze(2)          # (batch_size, num_pixels)\n",
        "        alpha = self.softmax(att)                    # (batch_size, num_pixels)\n",
        "        context = (features * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, feature_dim)\n",
        "        return context, alpha\n"
      ],
      "metadata": {
        "id": "MET8XDqB1aNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "You're building a decoder with attention that generates captions by dynamically focusing on different parts of the image at each word step."
      ],
      "metadata": {
        "id": "lNnX8ff1u07j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderWithAttention(nn.Module):\n",
        "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, feature_dim=2048, dropout=0.5):\n",
        "        super(DecoderWithAttention, self).__init__()\n",
        "        self.attention = Attention(feature_dim, decoder_dim, attention_dim)\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.decode_step = nn.LSTMCell(embed_dim + feature_dim, decoder_dim, bias=True)\n",
        "        self.init_h = nn.Linear(feature_dim, decoder_dim)\n",
        "        self.init_c = nn.Linear(feature_dim, decoder_dim)\n",
        "        self.f_beta = nn.Linear(decoder_dim, feature_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.fc = nn.Linear(decoder_dim, vocab_size)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def init_hidden_state(self, features):\n",
        "        mean_features = features.mean(dim=1)\n",
        "        h = self.init_h(mean_features)\n",
        "        c = self.init_c(mean_features)\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, features, captions, caption_lengths):\n",
        "        embeddings = self.embedding(captions)\n",
        "        h, c = self.init_hidden_state(features)\n",
        "\n",
        "        decode_lengths = caption_lengths - 1  # ✅ Fixed\n",
        "        max_decode_length = decode_lengths.max().item()\n",
        "\n",
        "        batch_size = features.size(0)\n",
        "        vocab_size = self.fc.out_features\n",
        "\n",
        "        predictions = torch.zeros(batch_size, max_decode_length, vocab_size).to(features.device)\n",
        "        alphas = torch.zeros(batch_size, max_decode_length, features.size(1)).to(features.device)\n",
        "\n",
        "        for t in range(max_decode_length):\n",
        "            batch_size_t = sum([l > t for l in decode_lengths])\n",
        "\n",
        "            context, alpha = self.attention(features[:batch_size_t], h[:batch_size_t])\n",
        "\n",
        "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar\n",
        "            context = gate * context\n",
        "\n",
        "            lstm_input = torch.cat([embeddings[:batch_size_t, t, :], context], dim=1)\n",
        "            h, c = self.decode_step(lstm_input, (h[:batch_size_t], c[:batch_size_t]))\n",
        "\n",
        "            preds = self.fc(self.dropout(h))\n",
        "            predictions[:batch_size_t, t, :] = preds\n",
        "            alphas[:batch_size_t, t, :] = alpha\n",
        "\n",
        "        return predictions, decode_lengths, alphas\n"
      ],
      "metadata": {
        "id": "ALqK0Hww1r5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're initializing the encoder and the new attention-based decoder models and moving them to your device (CPU or GPU)."
      ],
      "metadata": {
        "id": "9T8Ek61Ru9Vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models\n",
        "encoder = EncoderCNN().to(device)\n",
        "decoder = DecoderWithAttention(\n",
        "    attention_dim=512,\n",
        "    embed_dim=256,\n",
        "    decoder_dim=512,\n",
        "    vocab_size=vocab_size\n",
        ").to(device)\n"
      ],
      "metadata": {
        "id": "FbbAgHum13qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're setting up the loss function, optimizer, and gradient scaler (for mixed precision training) to prepare for model training."
      ],
      "metadata": {
        "id": "zKWb-tkwvK5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "optimizer = optim.Adam(list(decoder.parameters()) + list(encoder.parameters()), lr=1e-4)\n",
        "scaler = GradScaler()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "go8mn4X33v0G",
        "outputId": "8c02ec43-8085-4ff6-b298-a8d37eb1cf20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-89-ee2c218ae71c>:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're saving the trained encoder, decoder, and optimizer states into a checkpoint file for future loading or fine-tuning."
      ],
      "metadata": {
        "id": "CvzXcmCivQ9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save final model\n",
        "torch.save({\n",
        "    'encoder_state_dict': encoder.state_dict(),\n",
        "    'decoder_state_dict': decoder.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "}, '/content/best_checkpoint.pth')\n",
        "\n",
        "print(\"✅ Model saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jHXignJQJHq",
        "outputId": "96bc635e-8cee-432a-ca59-ecf75d0e3cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "You're defining a function to generate captions using beam search, which picks the best possible sequence of words instead of just the most likely next word at each step."
      ],
      "metadata": {
        "id": "wAHFNAuOvYAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_caption_beam_search(encoder, decoder, image, tokenizer, beam_size=5, max_len=50, device='cuda'):\n",
        "    k = beam_size\n",
        "    vocab_size = tokenizer.vocab_size\n",
        "\n",
        "    # Encode\n",
        "    encoder_out = encoder(image.unsqueeze(0).to(device))  # (1, feature_dim)\n",
        "    encoder_out = encoder_out.expand(k, encoder_out.size(1))  # (k, feature_dim)\n",
        "\n",
        "    # Initialize beams\n",
        "    seqs = torch.full((k, 1), tokenizer.cls_token_id, dtype=torch.long).to(device)  # CLS token\n",
        "    top_k_scores = torch.zeros(k, 1).to(device)\n",
        "\n",
        "    complete_seqs = []\n",
        "    complete_seqs_scores = []\n",
        "\n",
        "    # Initialize hidden states\n",
        "    h, c = decoder.init_hidden_state(encoder_out)\n",
        "\n",
        "    for step in range(max_len):\n",
        "        embeddings = decoder.embedding(seqs[:, -1])  # (k, embed_dim)\n",
        "        context, alpha = decoder.attention(encoder_out, h)  # (k, feature_dim)\n",
        "\n",
        "        gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar\n",
        "        context = gate * context\n",
        "\n",
        "        lstm_input = torch.cat([embeddings, context], dim=1)\n",
        "        h, c = decoder.decode_step(lstm_input, (h, c))\n",
        "\n",
        "        preds = decoder.fc(h)  # (k, vocab_size)\n",
        "        preds = F.log_softmax(preds, dim=1)\n",
        "\n",
        "        # Add log probabilities\n",
        "        scores = top_k_scores.expand_as(preds) + preds  # (k, vocab_size)\n",
        "\n",
        "        # Flatten for beam search\n",
        "        if step == 0:\n",
        "            top_k_scores, top_k_words = scores[0].topk(k, 0)\n",
        "        else:\n",
        "            top_k_scores, top_k_words = scores.view(-1).topk(k, 0)\n",
        "\n",
        "        prev_word_inds = top_k_words // vocab_size  # which beam\n",
        "        next_word_inds = top_k_words % vocab_size  # actual word\n",
        "\n",
        "        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)\n",
        "\n",
        "        # Check if completed\n",
        "        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if next_word != tokenizer.sep_token_id]\n",
        "        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
        "\n",
        "        if len(complete_inds) > 0:\n",
        "            complete_seqs.extend(seqs[complete_inds].tolist())\n",
        "            complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
        "\n",
        "        k -= len(complete_inds)  # reduce beam size accordingly\n",
        "\n",
        "        if k == 0:\n",
        "            break\n",
        "\n",
        "        # Proceed with incomplete sequences\n",
        "        seqs = seqs[incomplete_inds]\n",
        "        h = h[prev_word_inds[incomplete_inds]]\n",
        "        c = c[prev_word_inds[incomplete_inds]]\n",
        "        encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
        "        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
        "\n",
        "    if len(complete_seqs) == 0:\n",
        "        complete_seqs = seqs.tolist()\n",
        "        complete_seqs_scores = top_k_scores.squeeze(1)\n",
        "\n",
        "    i = torch.argmax(torch.tensor(complete_seqs_scores)).item()\n",
        "    best_seq = complete_seqs[i]\n",
        "\n",
        "    caption = tokenizer.decode(best_seq, skip_special_tokens=True)\n",
        "    return caption\n"
      ],
      "metadata": {
        "id": "m-bEZWB-uxYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're training the encoder-decoder model for 30 epochs, using mixed precision and scheduled sampling to gradually make the model rely less on ground-truth captions during training."
      ],
      "metadata": {
        "id": "C1VcEJxWvhiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import random\n",
        "import torch\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "scheduled_sampling_prob = 0.15  # 15%\n",
        "\n",
        "for epoch in range(30):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, captions in train_loader:\n",
        "        images = images.to(device)\n",
        "        captions = captions.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast(enabled=True):\n",
        "            encoder_out = encoder(images)\n",
        "            caption_lengths = (captions != tokenizer.pad_token_id).sum(dim=1)\n",
        "\n",
        "            outputs, decode_lengths, alphas = decoder(encoder_out, captions, caption_lengths)\n",
        "\n",
        "            targets = captions[:, 1:]  # Actual next tokens\n",
        "            batch_size, max_len = targets.size()\n",
        "\n",
        "            preds = outputs.clone()\n",
        "            sampled_preds = outputs.argmax(2)\n",
        "\n",
        "            # ✅ Make a clone of targets for Scheduled Sampling\n",
        "            sampled_targets = targets.clone()\n",
        "\n",
        "            for b in range(batch_size):\n",
        "                decode_len = decode_lengths[b]\n",
        "                for t in range(decode_len):\n",
        "                    if t > 0 and random.random() < scheduled_sampling_prob:\n",
        "                        sampled_targets[b, t] = sampled_preds[b, t-1]\n",
        "\n",
        "            outputs = outputs[:, :max(decode_lengths), :].contiguous()\n",
        "            outputs = outputs.view(-1, tokenizer.vocab_size)\n",
        "\n",
        "            sampled_targets = sampled_targets[:, :max(decode_lengths)].contiguous()\n",
        "            sampled_targets = sampled_targets.reshape(-1)\n",
        "\n",
        "            loss = criterion(outputs, sampled_targets)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f\"✅ Epoch [{epoch+1}/30], Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNJVgDGmu98S",
        "outputId": "60de9a80-1270-455b-834b-24bedaf136b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-103-df746eeb6b22>:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "<ipython-input-103-df746eeb6b22>:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch [1/30], Loss: 4.3215\n",
            "✅ Epoch [2/30], Loss: 4.2931\n",
            "✅ Epoch [3/30], Loss: 4.2760\n",
            "✅ Epoch [4/30], Loss: 4.2467\n",
            "✅ Epoch [5/30], Loss: 4.2595\n",
            "✅ Epoch [6/30], Loss: 4.2305\n",
            "✅ Epoch [7/30], Loss: 4.2335\n",
            "✅ Epoch [8/30], Loss: 4.2275\n",
            "✅ Epoch [9/30], Loss: 4.2143\n",
            "✅ Epoch [10/30], Loss: 4.2096\n",
            "✅ Epoch [11/30], Loss: 4.1901\n",
            "✅ Epoch [12/30], Loss: 4.1701\n",
            "✅ Epoch [13/30], Loss: 4.1722\n",
            "✅ Epoch [14/30], Loss: 4.1790\n",
            "✅ Epoch [15/30], Loss: 4.1698\n",
            "✅ Epoch [16/30], Loss: 4.1557\n",
            "✅ Epoch [17/30], Loss: 4.1539\n",
            "✅ Epoch [18/30], Loss: 4.1259\n",
            "✅ Epoch [19/30], Loss: 4.1408\n",
            "✅ Epoch [20/30], Loss: 4.1238\n",
            "✅ Epoch [21/30], Loss: 4.1273\n",
            "✅ Epoch [22/30], Loss: 4.1246\n",
            "✅ Epoch [23/30], Loss: 4.1066\n",
            "✅ Epoch [24/30], Loss: 4.0975\n",
            "✅ Epoch [25/30], Loss: 4.0920\n",
            "✅ Epoch [26/30], Loss: 4.0847\n",
            "✅ Epoch [27/30], Loss: 4.0714\n",
            "✅ Epoch [28/30], Loss: 4.0740\n",
            "✅ Epoch [29/30], Loss: 4.0633\n",
            "✅ Epoch [30/30], Loss: 4.0420\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're evaluating the model's caption quality on the validation set using BLEU scores (BLEU-1 to BLEU-4) and printing the results."
      ],
      "metadata": {
        "id": "t9hypId0vkgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bleu1, bleu2, bleu3, bleu4 = evaluate_bleu(encoder, decoder, val_loader, tokenizer)\n",
        "\n",
        "print(f\"✅ BLEU-1: {bleu1:.4f}\")\n",
        "print(f\"✅ BLEU-2: {bleu2:.4f}\")\n",
        "print(f\"✅ BLEU-3: {bleu3:.4f}\")\n",
        "print(f\"✅ BLEU-4: {bleu4:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfdQNW4dDIK1",
        "outputId": "9ae2ef24-ccb1-41f8-d146-f5e68cdcbe8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ BLEU-1: 0.1895\n",
            "✅ BLEU-2: 0.0974\n",
            "✅ BLEU-3: 0.0547\n",
            "✅ BLEU-4: 0.0312\n",
            "✅ BLEU-1: 0.1895\n",
            "✅ BLEU-2: 0.0974\n",
            "✅ BLEU-3: 0.0547\n",
            "✅ BLEU-4: 0.0312\n"
          ]
        }
      ]
    }
  ]
}